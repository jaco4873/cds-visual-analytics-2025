Model: MLPClassifier

Parameters:
  activation: relu
  alpha: 0.0001
  batch_size: 200
  beta_1: 0.9
  beta_2: 0.999
  early_stopping: True
  epsilon: 1e-08
  hidden_layer_sizes: (200, 100, 50)
  learning_rate: adaptive
  learning_rate_init: 0.001
  max_fun: 15000
  max_iter: 200
  momentum: 0.9
  n_iter_no_change: 10
  nesterovs_momentum: True
  power_t: 0.5
  random_state: None
  shuffle: True
  solver: adam
  tol: 0.0001
  validation_fraction: 0.1
  verbose: True
  warm_start: False

Number of iterations: 58
Number of layers: 5
Number of outputs: 10
Output activation: softmax
